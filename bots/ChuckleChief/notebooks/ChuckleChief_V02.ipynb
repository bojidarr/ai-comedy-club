{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TKbOd5G6kZPI",
   "metadata": {
    "id": "TKbOd5G6kZPI"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E-2QHtZSkZPP",
   "metadata": {
    "id": "E-2QHtZSkZPP"
   },
   "source": [
    "Humour, even for us humans, can be mysterious. It is no wonder that it poses a unique challenge for AI systems as well. Let us think about it - we all have that one friend who effortlessly brings laughter into our lives. They have a natural knack for timing, delivery, and a deep understanding of what makes something funny. But have we ever tried to explain why they are funny?\n",
    "\n",
    "In this project, we aim to tackle the challenging task of creating an AI bot that excels in generating new jokes. This task is particularly difficult due to the complexities of humour. Humour is subjective and context-dependent, making it challenging for an AI system to understand and replicate effectively.\n",
    "\n",
    "Additionally, jokes often rely on wordplay, sarcasm, and cultural references, which further complicates the task of generating original and funny jokes. Despite these challenges, we are determined to push the boundaries of AI and humour, striving to create a bot that can bring joy and laughter to users worldwide.\n",
    "\n",
    "Meet **ChuckleChief**, our enthusiastic and curious novice AI companion, eager to unravel the mysteries of humour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tj_evBmfQtyW",
   "metadata": {
    "id": "Tj_evBmfQtyW"
   },
   "source": [
    "### Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yxSNKfONQtJV",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:05:14.565204Z",
     "iopub.status.busy": "2023-07-30T02:05:14.564694Z",
     "iopub.status.idle": "2023-07-30T02:05:39.187190Z",
     "shell.execute_reply": "2023-07-30T02:05:39.185733Z",
     "shell.execute_reply.started": "2023-07-30T02:05:14.565168Z"
    },
    "id": "yxSNKfONQtJV"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U better_profanity datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e_p9tv2HSfH-",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:05:39.190100Z",
     "iopub.status.busy": "2023-07-30T02:05:39.189724Z",
     "iopub.status.idle": "2023-07-30T02:05:40.532423Z",
     "shell.execute_reply": "2023-07-30T02:05:40.531470Z",
     "shell.execute_reply.started": "2023-07-30T02:05:39.190062Z"
    },
    "id": "e_p9tv2HSfH-"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download([\"stopwords\", \"wordnet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ed353-4b1a-4468-896f-eb1e7b230b32",
   "metadata": {
    "id": "a68ed353-4b1a-4468-896f-eb1e7b230b32"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import better_profanity as bp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    IntervalStrategy,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MZvmcOZmkto8",
   "metadata": {
    "id": "MZvmcOZmkto8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c497e9d-1e4f-4aa8-8638-404c00da99fa",
   "metadata": {
    "id": "0c497e9d-1e4f-4aa8-8638-404c00da99fa"
   },
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e7d7c-14fb-4388-855f-711e8713a16d",
   "metadata": {
    "id": "c45e7d7c-14fb-4388-855f-711e8713a16d"
   },
   "source": [
    "| Dataset Name | Description | Source | Number of Jokes | Format |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Short Jokes | A collection of short jokes in English | Kaggle | 231,657 | CSV |\n",
    "| Joke Dataset - Stupidstuff | A dataset of English plaintext jokes from stupidstuff.org | GitHub | 3,770 | JSON |\n",
    "| Joke Dataset - Wocka | A dataset of English plaintext jokes from wocka.com | GitHub | 10,000 | JSON |\n",
    "\n",
    "The Short Jokes dataset from Kaggle is a large collection of short jokes in English that includes both one-liners and longer jokes. The dataset contains 231,657 jokes in CSV format.\n",
    "\n",
    "The Joke Dataset from taivop/joke-dataset on GitHub includes two separate datasets: Stupidstuff and Wocka. The Stupidstuff dataset contains 3,770 English plaintext jokes scraped from stupidstuff.org, while the Wocka dataset contains 10,000 English plaintext jokes scraped from wocka.com. Both datasets are in JSON format and contain additional fields such as category, title, and rating.\n",
    "\n",
    "Therefore, the total number of jokes in all three datasets combined is 245,427 jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec60167-c955-41c0-9441-5d16d4f9f1ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:46.121082Z",
     "iopub.status.busy": "2023-07-30T00:29:46.120360Z",
     "iopub.status.idle": "2023-07-30T00:29:46.960386Z",
     "shell.execute_reply": "2023-07-30T00:29:46.959294Z",
     "shell.execute_reply.started": "2023-07-30T00:29:46.121052Z"
    },
    "id": "aec60167-c955-41c0-9441-5d16d4f9f1ea"
   },
   "outputs": [],
   "source": [
    "def load_joke_data():\n",
    "    \"\"\"Load joke data\"\"\"\n",
    "    short_jokes_df = pd.read_csv(\"short_jokes.csv\")\n",
    "\n",
    "    with open(\"stupidstuff.json\", \"r\") as file:\n",
    "        stupidstuff = json.load(file)\n",
    "    stupidstuff_df = pd.DataFrame(stupidstuff, columns=[\"id\", \"body\", \"category\"])\n",
    "\n",
    "    with open(\"wocka.json\", \"r\") as file:\n",
    "        wocka = json.load(file)\n",
    "    wocka_df = pd.DataFrame(wocka, columns=[\"id\", \"title\", \"body\", \"category\"])\n",
    "\n",
    "    return short_jokes_df, stupidstuff_df, wocka_df\n",
    "\n",
    "\n",
    "short_jokes_df, stupidstuff_df, wocka_df = load_joke_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0a485-1c6d-44ca-a5ff-1a41735f27c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:46.962366Z",
     "iopub.status.busy": "2023-07-30T00:29:46.961996Z",
     "iopub.status.idle": "2023-07-30T00:29:46.981616Z",
     "shell.execute_reply": "2023-07-30T00:29:46.980543Z",
     "shell.execute_reply.started": "2023-07-30T00:29:46.962330Z"
    },
    "id": "31e0a485-1c6d-44ca-a5ff-1a41735f27c2"
   },
   "outputs": [],
   "source": [
    "short_jokes_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f7077-aeaa-4104-806f-ec9d1123531a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:46.983775Z",
     "iopub.status.busy": "2023-07-30T00:29:46.983403Z",
     "iopub.status.idle": "2023-07-30T00:29:46.994351Z",
     "shell.execute_reply": "2023-07-30T00:29:46.993138Z",
     "shell.execute_reply.started": "2023-07-30T00:29:46.983741Z"
    },
    "id": "7d1f7077-aeaa-4104-806f-ec9d1123531a"
   },
   "outputs": [],
   "source": [
    "stupidstuff_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543b6b8-a0e6-4a1a-88e8-cd9693425120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:46.996880Z",
     "iopub.status.busy": "2023-07-30T00:29:46.996225Z",
     "iopub.status.idle": "2023-07-30T00:29:47.009263Z",
     "shell.execute_reply": "2023-07-30T00:29:47.008208Z",
     "shell.execute_reply.started": "2023-07-30T00:29:46.996846Z"
    },
    "id": "5543b6b8-a0e6-4a1a-88e8-cd9693425120"
   },
   "outputs": [],
   "source": [
    "wocka_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6c537-19ba-4419-83b6-e00c9dd07c82",
   "metadata": {
    "id": "cdc6c537-19ba-4419-83b6-e00c9dd07c82"
   },
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e2074-d2c2-4565-aff5-b85ccef6ab14",
   "metadata": {
    "id": "581e2074-d2c2-4565-aff5-b85ccef6ab14"
   },
   "source": [
    "- Remove special characters.\n",
    "- Remove numbers.\n",
    "- Strip leading/trailing whitespace.\n",
    "- Remove newlines and carriage returns.\n",
    "- Check if the joke is clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9vWNzygkZPZ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:47.013360Z",
     "iopub.status.busy": "2023-07-30T00:29:47.013001Z",
     "iopub.status.idle": "2023-07-30T00:29:47.020468Z",
     "shell.execute_reply": "2023-07-30T00:29:47.019138Z",
     "shell.execute_reply.started": "2023-07-30T00:29:47.013332Z"
    },
    "id": "c9vWNzygkZPZ"
   },
   "outputs": [],
   "source": [
    "def preprocess_joke(joke):\n",
    "    \"\"\"Preprocess jokes\"\"\"\n",
    "    joke = re.sub(r\"[^\\w\\s.,?!]\", \"\", joke)\n",
    "    joke = re.sub(\"\\d\", \"\", joke)\n",
    "    joke = joke.strip()\n",
    "    joke = joke.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    return joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e967a4d-a2f9-4a15-a3b3-5c9b33e6f920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:48.127777Z",
     "iopub.status.busy": "2023-07-30T00:29:48.127136Z",
     "iopub.status.idle": "2023-07-30T00:29:51.763976Z",
     "shell.execute_reply": "2023-07-30T00:29:51.763011Z",
     "shell.execute_reply.started": "2023-07-30T00:29:48.127741Z"
    },
    "id": "4e967a4d-a2f9-4a15-a3b3-5c9b33e6f920"
   },
   "outputs": [],
   "source": [
    "short_jokes_df[\"Joke\"] = short_jokes_df[\"Joke\"].apply(preprocess_joke)\n",
    "stupidstuff_df[\"body\"] = stupidstuff_df[\"body\"].apply(preprocess_joke)\n",
    "wocka_df[\"body\"] = wocka_df[\"body\"].apply(preprocess_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ebfce-749a-4c4d-80cc-0d0380c7d759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:29:56.316580Z",
     "iopub.status.busy": "2023-07-30T00:29:56.316208Z",
     "iopub.status.idle": "2023-07-30T00:29:56.325120Z",
     "shell.execute_reply": "2023-07-30T00:29:56.323412Z",
     "shell.execute_reply.started": "2023-07-30T00:29:56.316552Z"
    },
    "id": "b47ebfce-749a-4c4d-80cc-0d0380c7d759"
   },
   "outputs": [],
   "source": [
    "short_jokes_df.shape, stupidstuff_df.shape, wocka_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02d5a3-5ed8-4223-8bd3-daa6ebadf2b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:30:01.249818Z",
     "iopub.status.busy": "2023-07-30T00:30:01.249446Z",
     "iopub.status.idle": "2023-07-30T00:30:01.274534Z",
     "shell.execute_reply": "2023-07-30T00:30:01.273232Z",
     "shell.execute_reply.started": "2023-07-30T00:30:01.249785Z"
    },
    "id": "fe02d5a3-5ed8-4223-8bd3-daa6ebadf2b9"
   },
   "outputs": [],
   "source": [
    "all_jokes = (\n",
    "    short_jokes_df[\"Joke\"].tolist()\n",
    "    + stupidstuff_df[\"body\"].tolist()\n",
    "    + wocka_df[\"body\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfd512-5240-4d25-a4a4-1204bd194f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:30:05.292377Z",
     "iopub.status.busy": "2023-07-30T00:30:05.291988Z",
     "iopub.status.idle": "2023-07-30T00:30:05.299118Z",
     "shell.execute_reply": "2023-07-30T00:30:05.298226Z",
     "shell.execute_reply.started": "2023-07-30T00:30:05.292346Z"
    },
    "id": "56dfd512-5240-4d25-a4a4-1204bd194f02"
   },
   "outputs": [],
   "source": [
    "print(len(all_jokes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SjzlCGp1kZPa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T00:31:50.085725Z",
     "iopub.status.busy": "2023-07-30T00:31:50.085346Z",
     "iopub.status.idle": "2023-07-30T00:31:50.099465Z",
     "shell.execute_reply": "2023-07-30T00:31:50.098394Z",
     "shell.execute_reply.started": "2023-07-30T00:31:50.085693Z"
    },
    "id": "SjzlCGp1kZPa"
   },
   "outputs": [],
   "source": [
    "def sample_jokes(all_jokes, n):\n",
    "    \"\"\"Sample jokes\"\"\"\n",
    "    joke_indexes = np.random.randint(0, len(all_jokes), n)\n",
    "    sampled_jokes = [all_jokes[index] for index in joke_indexes]\n",
    "    return sampled_jokes\n",
    "\n",
    "sampled_15000_jokes = sample_jokes(all_jokes, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c40035-5134-4647-ad67-a5816db1cec7",
   "metadata": {
    "id": "65c40035-5134-4647-ad67-a5816db1cec7"
   },
   "outputs": [],
   "source": [
    "def is_clean(joke):\n",
    "    \"\"\"Check if a joke is clean\"\"\"\n",
    "    if bp.profanity.contains_profanity(joke):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9dba4-968a-4ff0-baf6-ca4bc67a2a68",
   "metadata": {
    "id": "35f9dba4-968a-4ff0-baf6-ca4bc67a2a68"
   },
   "outputs": [],
   "source": [
    "def filter_jokes(jokes):\n",
    "    \"\"\"Filter out offensive jokes\"\"\"\n",
    "    clean_jokes = []\n",
    "    for joke in tqdm(jokes):\n",
    "        if is_clean(joke):\n",
    "            clean_jokes.append(joke)\n",
    "    return clean_jokes\n",
    "\n",
    "clean_jokes = filter_jokes(sampled_15000_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8n3-RHRCkZPb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-30T02:05:53.893594Z",
     "iopub.status.idle": "2023-07-30T02:05:53.894814Z",
     "shell.execute_reply": "2023-07-30T02:05:53.894574Z",
     "shell.execute_reply.started": "2023-07-30T02:05:53.894548Z"
    },
    "id": "8n3-RHRCkZPb"
   },
   "outputs": [],
   "source": [
    "len(clean_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Am_mbcC-kZPb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T01:12:18.742961Z",
     "iopub.status.busy": "2023-07-30T01:12:18.742592Z",
     "iopub.status.idle": "2023-07-30T01:12:18.833631Z",
     "shell.execute_reply": "2023-07-30T01:12:18.832694Z",
     "shell.execute_reply.started": "2023-07-30T01:12:18.742929Z"
    },
    "id": "Am_mbcC-kZPb"
   },
   "outputs": [],
   "source": [
    "clean_jokes_df = pd.DataFrame(clean_jokes, columns=[\"jokes\"])\n",
    "clean_jokes_df.to_csv(\"clean_jokes_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cCsJJ_fEkZPb",
   "metadata": {
    "id": "cCsJJ_fEkZPb"
   },
   "outputs": [],
   "source": [
    "clean_jokes_df = pd.read_csv(\"clean_jokes_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee67d7-8712-406f-97c1-0cd6a197128b",
   "metadata": {
    "id": "f1ee67d7-8712-406f-97c1-0cd6a197128b"
   },
   "outputs": [],
   "source": [
    "def get_subset(jokes_df, n):\n",
    "    \"\"\"Get data subset\"\"\"\n",
    "    clean_jokes = [\n",
    "        str(joke).strip() for joke in jokes_df[\"jokes\"] if len(str(joke).strip()) >= 10\n",
    "    ]\n",
    "    random_jokes = random.sample(clean_jokes, n)\n",
    "    subset_df = pd.DataFrame(random_jokes, columns=[\"jokes\"])\n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9795db-e2e8-479f-bcef-4af35ef1c880",
   "metadata": {
    "id": "ad9795db-e2e8-479f-bcef-4af35ef1c880"
   },
   "outputs": [],
   "source": [
    "def split_data(jokes_df, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"Split data\"\"\"\n",
    "    total_size = len(jokes_df)\n",
    "\n",
    "    # Calculate the number of examples for each set.\n",
    "    train_num = int(train_size * total_size)\n",
    "    val_num = int(val_size * total_size)\n",
    "    test_num = int(test_size * total_size)\n",
    "\n",
    "    all_indices = np.arange(total_size)\n",
    "    train_indices = np.random.choice(all_indices, train_num, replace=False)\n",
    "    val_indices = np.random.choice(\n",
    "        np.setdiff1d(all_indices, train_indices), val_num, replace=False\n",
    "    )\n",
    "    test_indices = np.setdiff1d(\n",
    "        all_indices, np.concatenate([train_indices, val_indices])\n",
    "    )\n",
    "\n",
    "    # Split data based on indices\n",
    "    train_df = jokes_df.iloc[train_indices]\n",
    "    val_df = jokes_df.iloc[val_indices]\n",
    "    test_df = jokes_df.iloc[test_indices]\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcedc6e-00a7-4a54-bd2c-9dcf836a02b9",
   "metadata": {
    "id": "9bcedc6e-00a7-4a54-bd2c-9dcf836a02b9"
   },
   "source": [
    "### Model Comparison\n",
    "\n",
    "- `bert-base-uncased`: BERT (Bidirectional Encoder Representations from Transformers) is a widely-used transformer-based model that has achieved state-of-the-art performance on various natural language processing tasks. The \"base\" variant refers to its medium-sized configuration, offering a balance between model size and performance. \"uncased\" indicates that the model treats all text as lowercase, disregarding capitalization. BERT incorporates a deep bidirectional transformer encoder, capturing contextual information from both preceding and following words. It is pretrained on a large corpus and can be fine-tuned for specific tasks.\n",
    "\n",
    "- `distilbert-base-uncased`: DistilBERT is a distilled version of BERT, striking a good balance between performance and efficiency. It retains competitive performance while being smaller and faster than the original BERT model. Like BERT, \"uncased\" signifies that the model operates with lowercase text. DistilBERT achieves efficiency gains through techniques such as knowledge distillation and parameter reduction. Its reduced size makes it more manageable and quicker to fine-tune, particularly in scenarios with limited computational resources or smaller training datasets.  \n",
    "\n",
    "- `gpt-2`: GPT-2 (Generative Pre-trained Transformer 2) is a cutting-edge language model explicitly designed for text generation tasks. Renowned for its ability to produce high-quality and coherent text, GPT-2 is particularly well-suited for joke generation. Built upon a transformer architecture with a substantial number of parameters, GPT-2 captures long-range dependencies in input text effectively.\n",
    "\n",
    "We will start with fine-tuning `gpt-2 (124M parameter)` model for joke generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pniYQE8pkZPc",
   "metadata": {
    "id": "pniYQE8pkZPc"
   },
   "source": [
    "### Load Model and Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W5ADFwD0e9qj",
   "metadata": {
    "id": "W5ADFwD0e9qj"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokeniser = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pgkx-7C0j4N7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:06:21.016951Z",
     "iopub.status.busy": "2023-07-30T02:06:21.016535Z",
     "iopub.status.idle": "2023-07-30T02:06:21.024561Z",
     "shell.execute_reply": "2023-07-30T02:06:21.022660Z",
     "shell.execute_reply.started": "2023-07-30T02:06:21.016916Z"
    },
    "id": "pgkx-7C0j4N7"
   },
   "outputs": [],
   "source": [
    "print(f\"The model has {sum(p.numel() for p in model.parameters()):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fAsq1zoOkZPd",
   "metadata": {
    "id": "fAsq1zoOkZPd"
   },
   "source": [
    "### Add Custom Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c8a5a-9467-438c-a14d-c88f7633772f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:06:21.040014Z",
     "iopub.status.busy": "2023-07-30T02:06:21.039405Z",
     "iopub.status.idle": "2023-07-30T02:06:23.749590Z",
     "shell.execute_reply": "2023-07-30T02:06:23.748484Z",
     "shell.execute_reply.started": "2023-07-30T02:06:21.039975Z"
    },
    "id": "750c8a5a-9467-438c-a14d-c88f7633772f"
   },
   "outputs": [],
   "source": [
    "bos = \"<|endoftext|>\"  # Beginning of sequence token\n",
    "eos = \"<|eos|>\"        # End of sequence token\n",
    "pad = \"<|pad|>\"        # Padding token\n",
    "\n",
    "special_tokens = {\"bos_token\": bos, \"eos_token\": eos, \"pad_token\": pad}\n",
    "\n",
    "# Add custom tokens to the tokeniser.\n",
    "new_tokens = tokeniser.add_special_tokens(special_tokens)\n",
    "\n",
    "# Model config with custom tokens.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    bos_token_id=tokeniser.bos_token_id,\n",
    "    eos_token_id=tokeniser.eos_token_id,\n",
    "    pad_token_id=tokeniser.pad_token_id,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Load model with config.\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "# Resize embeddings to include new tokens.\n",
    "model.resize_token_embeddings(len(tokeniser))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7keSNGkMkZPd",
   "metadata": {
    "id": "7keSNGkMkZPd"
   },
   "source": [
    "### Define a Helper Function to Generate Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PRPrh1ioY5b5",
   "metadata": {
    "id": "PRPrh1ioY5b5"
   },
   "outputs": [],
   "source": [
    "def generate_jokes(model, tokeniser, prompt, num_jokes=1, max_len=50):\n",
    "    filtered_jokes = []\n",
    "\n",
    "    while len(filtered_jokes) < num_jokes:\n",
    "        input_ids = tokeniser.encode(prompt, return_tensors=\"pt\")\n",
    "        generated_text_samples = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_len,\n",
    "            num_return_sequences=num_jokes,\n",
    "            repetition_penalty=1.2,\n",
    "            temperature=0.75,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        generated_jokes = [\n",
    "            tokeniser.decode(joke, skip_special_tokens=True)\n",
    "            for joke in generated_text_samples\n",
    "        ]\n",
    "\n",
    "        # Apply filter to keep only inoffensive jokes.\n",
    "        for joke in generated_jokes:\n",
    "            if is_clean(joke):\n",
    "                filtered_jokes.append(joke)\n",
    "\n",
    "    return filtered_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67582d5c-d43f-4d5d-b0aa-88f6e1191449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:06:31.845684Z",
     "iopub.status.busy": "2023-07-30T02:06:31.845224Z",
     "iopub.status.idle": "2023-07-30T02:06:31.895586Z",
     "shell.execute_reply": "2023-07-30T02:06:31.894604Z",
     "shell.execute_reply.started": "2023-07-30T02:06:31.845635Z"
    },
    "id": "67582d5c-d43f-4d5d-b0aa-88f6e1191449"
   },
   "outputs": [],
   "source": [
    "clean_jokes_subset = get_subset(clean_jokes_df, 11000)\n",
    "train_df, val_df, test_df = split_data(clean_jokes_subset)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f245d-8852-465e-9d64-98344abb9d12",
   "metadata": {
    "id": "c60f245d-8852-465e-9d64-98344abb9d12"
   },
   "outputs": [],
   "source": [
    "train_jokes = Dataset.from_pandas(train_df[[\"jokes\"]])\n",
    "val_jokes = Dataset.from_pandas(val_df[[\"jokes\"]])\n",
    "test_jokes = Dataset.from_pandas(test_df[[\"jokes\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke4TzOa6kZPk",
   "metadata": {
    "id": "ke4TzOa6kZPk"
   },
   "source": [
    "### Tokenise and Pad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f0ecb-c357-4dd9-a4be-cc2c14577a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:06:34.857253Z",
     "iopub.status.busy": "2023-07-30T02:06:34.856890Z",
     "iopub.status.idle": "2023-07-30T02:06:49.115522Z",
     "shell.execute_reply": "2023-07-30T02:06:49.114367Z",
     "shell.execute_reply.started": "2023-07-30T02:06:34.857223Z"
    },
    "id": "7c6f0ecb-c357-4dd9-a4be-cc2c14577a90"
   },
   "outputs": [],
   "source": [
    "def tokenise_data(data, max_length=512):\n",
    "    return data.map(\n",
    "        lambda x: tokeniser(\n",
    "            x[\"jokes\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "tokenised_train_jokes = tokenise_data(train_jokes)\n",
    "tokenised_val_jokes = tokenise_data(val_jokes)\n",
    "tokenised_test_jokes = tokenise_data(test_jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W7DFY1QhkZPl",
   "metadata": {
    "id": "W7DFY1QhkZPl"
   },
   "source": [
    "### Set up the Training Arguments and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb674eb-68eb-4f85-8f50-d6a016e5b3dd",
   "metadata": {
    "id": "8eb674eb-68eb-4f85-8f50-d6a016e5b3dd"
   },
   "outputs": [],
   "source": [
    "model_path = \"./ChuckleChief\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=model_path,\n",
    "    prediction_loss_only=True,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=500,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokeniser, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dvo7S87-kZPl",
   "metadata": {
    "id": "dvo7S87-kZPl"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sLDUi2TAnWhr",
   "metadata": {
    "id": "sLDUi2TAnWhr"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4DMR3qDDgMBU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "execution": {
     "iopub.execute_input": "2023-07-30T02:06:49.144448Z",
     "iopub.status.busy": "2023-07-30T02:06:49.144093Z",
     "iopub.status.idle": "2023-07-30T02:06:58.923371Z",
     "shell.execute_reply": "2023-07-30T02:06:58.921598Z",
     "shell.execute_reply.started": "2023-07-30T02:06:49.144399Z"
    },
    "id": "4DMR3qDDgMBU",
    "outputId": "6218ba55-3e1a-4994-a0a3-ecdc656b8a9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4400' max='4400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4400/4400 1:31:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.303400</td>\n",
       "      <td>4.218644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.233000</td>\n",
       "      <td>4.113698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.008900</td>\n",
       "      <td>4.073078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.984900</td>\n",
       "      <td>4.045985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.855800</td>\n",
       "      <td>4.033286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.801600</td>\n",
       "      <td>4.024386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.764100</td>\n",
       "      <td>4.024865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.719600</td>\n",
       "      <td>4.019519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4400, training_loss=4.164508556019176, metrics={'train_runtime': 5510.0658, 'train_samples_per_second': 6.388, 'train_steps_per_second': 0.799, 'total_flos': 9197479526400000.0, 'train_loss': 4.164508556019176, 'epoch': 4.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenised_train_jokes,\n",
    "    eval_dataset=tokenised_val_jokes,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JG4hGb3JkZPm",
   "metadata": {
    "id": "JG4hGb3JkZPm"
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1448282-d519-4936-9758-0a86cd0df9f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "b1448282-d519-4936-9758-0a86cd0df9f4",
    "outputId": "45a55039-0332-4dfa-f465-c195d9289fdf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 00:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.97096848487854,\n",
       " 'eval_runtime': 53.2835,\n",
       " 'eval_samples_per_second': 20.644,\n",
       " 'eval_steps_per_second': 2.59,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenised_test_jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8lrWsy0kZPl",
   "metadata": {
    "id": "i8lrWsy0kZPl"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14836837-d710-454e-ba31-ff5880e6dccc",
   "metadata": {
    "id": "14836837-d710-454e-ba31-ff5880e6dccc"
   },
   "outputs": [],
   "source": [
    "new_model_path = os.path.join(model_path, \"model\")\n",
    "if not os.path.exists(new_model_path):\n",
    "    os.makedirs(new_model_path)\n",
    "\n",
    "trainer.save_model(new_model_path)\n",
    "tokeniser.save_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7svR2lVBkZPm",
   "metadata": {
    "id": "7svR2lVBkZPm"
   },
   "source": [
    "### Generate Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062d376-4f85-489f-abbc-015afcabebcd",
   "metadata": {
    "id": "2062d376-4f85-489f-abbc-015afcabebcd"
   },
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained(new_model_path)\n",
    "loaded_tokeniser = GPT2Tokenizer.from_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823fa7d1-8dcc-4650-a740-35fd41aeecf9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "823fa7d1-8dcc-4650-a740-35fd41aeecf9",
    "outputId": "3b94da72-79e3-4d99-e4b3-49b18ff78911"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is a joke filled with harmless humour: You know how it goes when you cross the road. If they hit your car, Im going to tell',\n",
       " 'Here is a joke filled with harmless humour: A horse walks into the bar... And looks at him, his face just reddens up.  Which',\n",
       " 'Here is a joke filled with harmless humour: When you talk about the Internet, how can I get around it? Unless they allow me to run away']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Here is a joke filled with harmless humour:\"\n",
    "jokes = generate_jokes(loaded_model, loaded_tokeniser, prompt, num_jokes=3, max_len=30)\n",
    "jokes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92e5a817-fe77-4d5e-b833-3b0fffb366c5",
   "metadata": {
    "id": "dd1b5789-ea81-4ae1-bfff-e1eda5a3fda9"
   },
   "source": [
    "### Acknowledgement: Datasets\n",
    "\n",
    "#### [Short Jokes](https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes)\n",
    "#### [Stupidstuff](https://github.com/taivop/joke-dataset#stupidstuffjson)\n",
    "#### [Wocka](https://github.com/taivop/joke-dataset#wockajson)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
